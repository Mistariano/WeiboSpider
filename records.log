2015-04-24 13:57:47+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 13:57:47+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 13:57:47+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 13:57:47+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 13:57:48+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 13:57:48+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 13:57:48+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 13:57:48+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 13:59:12+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 13:59:12+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 13:59:12+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 13:59:12+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 13:59:13+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 13:59:13+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 13:59:13+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 13:59:13+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 14:00:09+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 14:00:09+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 14:00:09+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 14:00:09+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 14:00:10+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 14:00:10+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 14:00:10+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 14:00:10+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 14:00:56+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 14:00:56+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 14:00:56+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 14:00:56+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 14:00:57+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 14:00:57+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 14:00:57+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 14:00:57+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 14:01:46+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 14:01:46+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 14:01:46+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 14:01:46+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 14:01:47+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 14:01:47+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 14:01:47+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 14:01:47+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 14:02:29+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 14:02:29+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 14:02:29+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 14:02:29+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 14:02:30+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 14:02:30+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 14:02:30+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 14:02:30+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 14:48:42+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 14:48:42+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 14:48:42+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 14:48:42+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 14:48:43+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 14:48:43+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 14:48:43+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 14:48:43+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 14:49:21+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 14:49:21+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 14:49:21+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 14:49:21+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 14:49:22+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 14:49:22+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 14:49:22+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 14:49:22+0800 [scrapy] ERROR: 'NoneType' object has no attribute 'keys'
2015-04-24 15:13:01+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 15:13:01+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 15:13:01+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 15:13:01+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 15:13:02+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 15:13:02+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 15:13:02+0800 [scrapy] INFO: Enabled item pipelines: WeiboCrawlerPipeline
2015-04-24 15:13:02+0800 [weiboSpider] INFO: Spider opened
2015-04-24 15:13:02+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:13:03+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 15:13:03+0800 [scrapy] ERROR: fe_sendauth: no password supplied
	
2015-04-24 15:13:03+0800 [-] Unhandled Error
	Traceback (most recent call last):
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 93, in start
	    self.start_reactor()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 130, in start_reactor
	    reactor.run(installSignalHandlers=False)  # blocking call
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1192, in run
	    self.mainLoop()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	--- <exception caught here> ---
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/utils/reactor.py", line 41, in __call__
	    return self._func(*self._a, **self._kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/core/engine.py", line 112, in _next_request
	    request = next(slot.start_requests)
	  File "/Users/liuyi103/PyCharmProjects/sina_weibo_crawler/weibo_crawler/spiders/weiboSpider.py", line 37, in start_requests
	    sys.exit("Crawler Exit.")
	exceptions.SystemExit: Crawler Exit.
	
2015-04-24 15:14:02+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:15:02+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:27:47+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 15:27:47+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 15:27:47+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 15:27:48+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 15:27:49+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 15:27:49+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 15:27:49+0800 [scrapy] INFO: Enabled item pipelines: WeiboCrawlerPipeline
2015-04-24 15:27:49+0800 [weiboSpider] INFO: Spider opened
2015-04-24 15:27:49+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:27:49+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 15:27:49+0800 [scrapy] ERROR: fe_sendauth: no password supplied
	
2015-04-24 15:27:49+0800 [-] Unhandled Error
	Traceback (most recent call last):
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 93, in start
	    self.start_reactor()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 130, in start_reactor
	    reactor.run(installSignalHandlers=False)  # blocking call
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1192, in run
	    self.mainLoop()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	--- <exception caught here> ---
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/utils/reactor.py", line 41, in __call__
	    return self._func(*self._a, **self._kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/core/engine.py", line 112, in _next_request
	    request = next(slot.start_requests)
	  File "/Users/liuyi103/PyCharmProjects/sina_weibo_crawler/weibo_crawler/spiders/weiboSpider.py", line 37, in start_requests
	    sys.exit("Crawler Exit.")
	exceptions.SystemExit: Crawler Exit.
	
2015-04-24 15:34:43+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 15:34:43+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 15:34:43+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 15:34:43+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 15:34:44+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 15:34:44+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 15:34:44+0800 [scrapy] INFO: Enabled item pipelines: WeiboCrawlerPipeline
2015-04-24 15:34:44+0800 [weiboSpider] INFO: Spider opened
2015-04-24 15:34:44+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:34:44+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 15:34:44+0800 [scrapy] ERROR: fe_sendauth: no password supplied
	
2015-04-24 15:34:44+0800 [-] Unhandled Error
	Traceback (most recent call last):
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 93, in start
	    self.start_reactor()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 130, in start_reactor
	    reactor.run(installSignalHandlers=False)  # blocking call
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1192, in run
	    self.mainLoop()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	--- <exception caught here> ---
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/utils/reactor.py", line 41, in __call__
	    return self._func(*self._a, **self._kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/core/engine.py", line 112, in _next_request
	    request = next(slot.start_requests)
	  File "/Users/liuyi103/PyCharmProjects/sina_weibo_crawler/weibo_crawler/spiders/weiboSpider.py", line 37, in start_requests
	    sys.exit("Crawler Exit.")
	exceptions.SystemExit: Crawler Exit.
	
2015-04-24 15:35:44+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:36:44+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:37:44+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:38:44+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:39:41+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 15:39:41+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 15:39:41+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 15:39:41+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 15:39:42+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 15:39:42+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 15:39:42+0800 [scrapy] INFO: Enabled item pipelines: WeiboCrawlerPipeline
2015-04-24 15:39:42+0800 [weiboSpider] INFO: Spider opened
2015-04-24 15:39:42+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:39:42+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 15:39:42+0800 [scrapy] ERROR: fe_sendauth: no password supplied
	
2015-04-24 15:39:42+0800 [-] Unhandled Error
	Traceback (most recent call last):
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 93, in start
	    self.start_reactor()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 130, in start_reactor
	    reactor.run(installSignalHandlers=False)  # blocking call
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1192, in run
	    self.mainLoop()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	--- <exception caught here> ---
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/utils/reactor.py", line 41, in __call__
	    return self._func(*self._a, **self._kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/core/engine.py", line 112, in _next_request
	    request = next(slot.start_requests)
	  File "/Users/liuyi103/PyCharmProjects/sina_weibo_crawler/weibo_crawler/spiders/weiboSpider.py", line 37, in start_requests
	    sys.exit("Crawler Exit.")
	exceptions.SystemExit: Crawler Exit.
	
2015-04-24 15:40:42+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:41:40+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 15:41:40+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 15:41:40+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 15:41:40+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 15:41:41+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 15:41:41+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 15:41:41+0800 [scrapy] INFO: Enabled item pipelines: WeiboCrawlerPipeline
2015-04-24 15:41:41+0800 [weiboSpider] INFO: Spider opened
2015-04-24 15:41:41+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:41:41+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 15:41:41+0800 [scrapy] ERROR: fe_sendauth: no password supplied
	
2015-04-24 15:41:41+0800 [-] Unhandled Error
	Traceback (most recent call last):
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 93, in start
	    self.start_reactor()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 130, in start_reactor
	    reactor.run(installSignalHandlers=False)  # blocking call
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1192, in run
	    self.mainLoop()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	--- <exception caught here> ---
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/utils/reactor.py", line 41, in __call__
	    return self._func(*self._a, **self._kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/core/engine.py", line 112, in _next_request
	    request = next(slot.start_requests)
	  File "/Users/liuyi103/PyCharmProjects/sina_weibo_crawler/weibo_crawler/spiders/weiboSpider.py", line 37, in start_requests
	    sys.exit("Crawler Exit.")
	exceptions.SystemExit: Crawler Exit.
	
2015-04-24 15:42:41+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:43:41+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:44:41+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:45:41+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:46:41+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:47:22+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 15:47:22+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 15:47:22+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 15:47:22+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 15:47:23+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 15:47:23+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 15:47:23+0800 [scrapy] INFO: Enabled item pipelines: WeiboCrawlerPipeline
2015-04-24 15:47:23+0800 [weiboSpider] INFO: Spider opened
2015-04-24 15:47:23+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:47:23+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 15:47:23+0800 [scrapy] ERROR: fe_sendauth: no password supplied
	
2015-04-24 15:47:23+0800 [-] Unhandled Error
	Traceback (most recent call last):
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 93, in start
	    self.start_reactor()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 130, in start_reactor
	    reactor.run(installSignalHandlers=False)  # blocking call
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1192, in run
	    self.mainLoop()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	--- <exception caught here> ---
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/utils/reactor.py", line 41, in __call__
	    return self._func(*self._a, **self._kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/core/engine.py", line 112, in _next_request
	    request = next(slot.start_requests)
	  File "/Users/liuyi103/PyCharmProjects/sina_weibo_crawler/weibo_crawler/spiders/weiboSpider.py", line 37, in start_requests
	    sys.exit("Crawler Exit.")
	exceptions.SystemExit: Crawler Exit.
	
2015-04-24 15:48:23+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:49:23+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:50:23+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:51:15+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-24 15:51:15+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-24 15:51:15+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-24 15:51:15+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-24 15:51:16+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-24 15:51:16+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-24 15:51:16+0800 [scrapy] INFO: Enabled item pipelines: WeiboCrawlerPipeline
2015-04-24 15:51:16+0800 [weiboSpider] INFO: Spider opened
2015-04-24 15:51:16+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:51:16+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-24 15:51:16+0800 [scrapy] ERROR: fe_sendauth: no password supplied
	
2015-04-24 15:51:16+0800 [-] Unhandled Error
	Traceback (most recent call last):
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 93, in start
	    self.start_reactor()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/crawler.py", line 130, in start_reactor
	    reactor.run(installSignalHandlers=False)  # blocking call
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1192, in run
	    self.mainLoop()
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	--- <exception caught here> ---
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Twisted-14.0.2-py2.7-macosx-10.5-x86_64.egg/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/utils/reactor.py", line 41, in __call__
	    return self._func(*self._a, **self._kw)
	  File "/Users/liuyi103/anaconda/lib/python2.7/site-packages/Scrapy-0.24.4-py2.7.egg/scrapy/core/engine.py", line 112, in _next_request
	    request = next(slot.start_requests)
	  File "/Users/liuyi103/PyCharmProjects/sina_weibo_crawler/weibo_crawler/spiders/weiboSpider.py", line 37, in start_requests
	    sys.exit("Crawler Exit.")
	exceptions.SystemExit: Crawler Exit.
	
2015-04-24 15:52:16+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:53:16+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:54:16+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-24 15:55:16+0800 [weiboSpider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-27 11:05:10+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-27 11:05:10+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-27 11:05:10+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-27 11:05:10+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-27 11:05:11+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-27 11:05:11+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-27 11:05:11+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-27 11:05:11+0800 [scrapy] ERROR: (2002, "Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)")
2015-04-27 11:06:29+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-27 11:06:29+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-27 11:06:29+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-27 11:06:29+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-27 11:06:30+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-27 11:06:30+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-27 11:06:30+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-27 11:06:30+0800 [scrapy] ERROR: (2002, "Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)")
2015-04-27 11:08:55+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-27 11:08:55+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-27 11:08:55+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-27 11:08:55+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-27 11:08:56+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-27 11:08:56+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-27 11:08:56+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-27 11:08:56+0800 [scrapy] ERROR: (2002, "Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)")
2015-04-27 11:09:29+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-27 11:09:29+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-27 11:09:29+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-27 11:09:29+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-27 11:09:30+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-27 11:09:30+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-27 11:09:30+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-27 11:09:30+0800 [scrapy] ERROR: (2002, "Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)")
2015-04-27 11:09:53+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-27 11:09:53+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-27 11:09:53+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-27 11:09:53+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-27 11:09:54+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-27 11:09:54+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-27 11:09:54+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-27 11:09:54+0800 [scrapy] ERROR: (2002, "Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)")
2015-04-27 11:10:27+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: weibo_crawler)
2015-04-27 11:10:27+0800 [scrapy] INFO: Optional features available: ssl, http11, boto, django
2015-04-27 11:10:27+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'LOG_FILE': 'records.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'weibo_crawler'}
2015-04-27 11:10:27+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-27 11:10:27+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-27 11:10:27+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-27 11:10:27+0800 [scrapy] ERROR: Unable to connect to the database.
2015-04-27 11:10:27+0800 [scrapy] ERROR: (2002, "Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)")
2015-08-12 22:19:05 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:19:05 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:19:05 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'FEED_URI': 'i.json', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'FEED_FORMAT': 'json', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:19:05 [scrapy] INFO: Enabled extensions: CloseSpider, FeedExporter, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:19:06 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:19:06 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:19:06 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:19:06 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:19:06 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:19:06 [scrapy] INFO: Spider opened
2015-08-12 22:19:06 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:19:06 [twisted] ERROR: Unhandled Error
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "D:\Anaconda\lib\site-packages\scrapy\crawler.py", line 251, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "D:\Anaconda\lib\site-packages\twisted\internet\base.py", line 1192, in run
    self.mainLoop()
  File "D:\Anaconda\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "D:\Anaconda\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "D:\Anaconda\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 17, in start_requests
    sys.exit("Error:cannot login,crawler exit.")
exceptions.SystemExit: Error:cannot login,crawler exit.

2015-08-12 22:20:06 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:35:22 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:35:22 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:35:22 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:35:22 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:35:23 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:35:23 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:35:23 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:35:23 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:35:23 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:35:23 [scrapy] INFO: Spider opened
2015-08-12 22:35:23 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:35:23 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 18, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie, callback=self.parse_tohot)
NameError: global name 'Request' is not defined
2015-08-12 22:35:23 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:35:23 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 35, 23, 452000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 35, 23, 437000)}
2015-08-12 22:35:23 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:38:54 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:38:54 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:38:54 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:38:54 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:38:56 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:38:56 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:38:56 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:38:56 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:38:56 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:38:56 [scrapy] INFO: Spider opened
2015-08-12 22:38:56 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:38:56 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie, callback=self.parse_tohot)
NameError: global name 'Request' is not defined
2015-08-12 22:38:56 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:38:56 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 38, 56, 527000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 38, 56, 512000)}
2015-08-12 22:38:56 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:40:11 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:40:11 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:40:11 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:40:11 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:40:13 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:40:13 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:40:13 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:40:13 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:40:13 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:40:13 [scrapy] INFO: Spider opened
2015-08-12 22:40:13 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:40:14 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie, callback=self.parse_tohot)
NameError: global name 'Request' is not defined
2015-08-12 22:40:14 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:40:14 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 40, 14, 279000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 40, 13, 662000)}
2015-08-12 22:40:14 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:44:06 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:44:06 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:44:06 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:44:06 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:44:08 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:44:08 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:44:08 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:44:08 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:44:08 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:44:08 [scrapy] INFO: Spider opened
2015-08-12 22:44:08 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:44:08 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie, callback=self.parse_tohot)
NameError: global name 'Request' is not defined
2015-08-12 22:44:08 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:44:08 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 44, 8, 780000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 44, 8, 749000)}
2015-08-12 22:44:08 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:47:15 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:47:15 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:47:15 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:47:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:47:17 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:47:17 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:47:17 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:47:17 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:47:17 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:47:17 [scrapy] INFO: Spider opened
2015-08-12 22:47:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:47:17 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie, callback=self.parse)
NameError: global name 'Request' is not defined
2015-08-12 22:47:17 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:47:17 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 47, 17, 116000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 47, 17, 100000)}
2015-08-12 22:47:17 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:47:35 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:47:35 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:47:35 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:47:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:47:37 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:47:37 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:47:37 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:47:37 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:47:37 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:47:37 [scrapy] INFO: Spider opened
2015-08-12 22:47:37 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:47:37 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie)
NameError: global name 'Request' is not defined
2015-08-12 22:47:37 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:47:37 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 47, 37, 249000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 47, 37, 218000)}
2015-08-12 22:47:37 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:48:35 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:48:35 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:48:35 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:48:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:48:36 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:48:36 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:48:36 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:48:36 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:48:36 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:48:36 [scrapy] INFO: Spider opened
2015-08-12 22:48:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:48:36 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie)
NameError: global name 'Request' is not defined
2015-08-12 22:48:36 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:48:36 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 48, 36, 918000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 48, 36, 903000)}
2015-08-12 22:48:36 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:50:43 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:50:43 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:50:43 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:50:43 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:50:44 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:50:44 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:50:44 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:50:44 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:50:44 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:50:44 [scrapy] INFO: Spider opened
2015-08-12 22:50:44 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:50:44 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie,callback=parse)
NameError: global name 'Request' is not defined
2015-08-12 22:50:44 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:50:44 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 50, 44, 703000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 50, 44, 677000)}
2015-08-12 22:50:44 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:52:07 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:52:07 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:52:07 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:52:07 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:52:09 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:52:09 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:52:09 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:52:09 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:52:09 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:52:09 [scrapy] INFO: Spider opened
2015-08-12 22:52:09 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:52:09 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url, cookies=self.login_cookie,callback=parse)
NameError: global name 'Request' is not defined
2015-08-12 22:52:09 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:52:09 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 52, 9, 150000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 52, 9, 117000)}
2015-08-12 22:52:09 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:52:52 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:52:52 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:52:52 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:52:52 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:52:54 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:52:54 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:52:54 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:52:54 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:52:54 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:52:54 [scrapy] INFO: Spider opened
2015-08-12 22:52:54 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:52:54 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url)
NameError: global name 'Request' is not defined
2015-08-12 22:52:54 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:52:54 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 52, 54, 132000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 52, 54, 102000)}
2015-08-12 22:52:54 [scrapy] INFO: Spider closed (finished)
2015-08-12 22:55:12 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 22:55:12 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 22:55:12 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 22:55:12 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 22:55:13 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 22:55:13 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 22:55:13 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 22:55:13 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 22:55:13 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 22:55:13 [scrapy] INFO: Spider opened
2015-08-12 22:55:13 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 22:55:14 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url)
NameError: global name 'Request' is not defined
2015-08-12 22:55:14 [scrapy] INFO: Closing spider (finished)
2015-08-12 22:55:14 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 14, 55, 14, 94000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 14, 55, 13, 750000)}
2015-08-12 22:55:14 [scrapy] INFO: Spider closed (finished)
2015-08-12 23:00:46 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 23:00:46 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 23:00:46 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 23:00:46 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 23:00:47 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 23:00:47 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 23:00:47 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 23:00:47 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 23:00:47 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 23:00:47 [scrapy] INFO: Spider opened
2015-08-12 23:00:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 23:00:47 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 20, in start_requests
    yield Request(url=hot_list_url)
NameError: global name 'Request' is not defined
2015-08-12 23:00:47 [scrapy] INFO: Closing spider (finished)
2015-08-12 23:00:47 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 15, 0, 47, 907000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 15, 0, 47, 888000)}
2015-08-12 23:00:47 [scrapy] INFO: Spider closed (finished)
2015-08-12 23:03:06 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-12 23:03:06 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-12 23:03:06 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-12 23:03:06 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-12 23:03:08 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-12 23:03:08 [boto] ERROR: Unable to read instance data, giving up
2015-08-12 23:03:08 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-12 23:03:08 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-12 23:03:08 [scrapy] INFO: Enabled item pipelines: 
2015-08-12 23:03:08 [scrapy] INFO: Spider opened
2015-08-12 23:03:08 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-12 23:03:08 [scrapy] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 110, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\lenovo\Desktop\sina - mist\weibo_crawler\spiders\weiboSpider.py", line 22, in start_requests
    yield Request(url=hot_list_url)
NameError: global name 'hot_list_url' is not defined
2015-08-12 23:03:08 [scrapy] INFO: Closing spider (finished)
2015-08-12 23:03:08 [scrapy] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 12, 15, 3, 8, 218000),
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2015, 8, 12, 15, 3, 8, 194000)}
2015-08-12 23:03:08 [scrapy] INFO: Spider closed (finished)
2015-08-13 09:48:07 [scrapy] INFO: Scrapy 1.0.2 started (bot: weibo_crawler)
2015-08-13 09:48:07 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-08-13 09:48:07 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'weibo_crawler.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['weibo_crawler.spiders'], 'BOT_NAME': 'weibo_crawler', 'LOG_FILE': 'records.log', 'DOWNLOAD_DELAY': 0.25}
2015-08-13 09:48:07 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-08-13 09:48:09 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\boto\utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "D:\Anaconda\lib\urllib2.py", line 431, in open
    response = self._open(req, data)
  File "D:\Anaconda\lib\urllib2.py", line 449, in _open
    '_open', req)
  File "D:\Anaconda\lib\urllib2.py", line 409, in _call_chain
    result = func(*args)
  File "D:\Anaconda\lib\urllib2.py", line 1227, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "D:\Anaconda\lib\urllib2.py", line 1197, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2015-08-13 09:48:09 [boto] ERROR: Unable to read instance data, giving up
2015-08-13 09:48:09 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-08-13 09:48:09 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-08-13 09:48:09 [scrapy] INFO: Enabled item pipelines: 
2015-08-13 09:48:09 [scrapy] INFO: Spider opened
2015-08-13 09:48:09 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-13 09:48:10 [scrapy] INFO: Closing spider (finished)
2015-08-13 09:48:10 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1617,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 27285,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 8, 13, 1, 48, 10, 342000),
 'item_scraped_count': 1,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 8, 13, 1, 48, 9, 92000)}
2015-08-13 09:48:10 [scrapy] INFO: Spider closed (finished)
